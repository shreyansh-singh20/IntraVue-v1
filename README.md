# IntraVue v1 ğŸ¯  
AI-Powered Interview Evaluation System

IntraVue is an AI-driven interview intelligence platform that evaluates a candidateâ€™s answer to a single interview question and provides **structured, unbiased, and actionable feedback** in real time.

This repository contains **IntraVue v1**, the **foundational version** of the system, focused on **prompt-engineered GenAI evaluation with a polished frontend experience**.

---

## ğŸš€ What is IntraVue v1?

**IntraVue v1** is a single-question interview evaluator that:

- Accepts job context and candidate answers
- Uses a Large Language Model (LLM) with strict prompt formatting
- Produces structured interview feedback
- Visualizes results with confidence scoring and UX polish

> âš ï¸ This version is **not RAG-based** and does **not use agents**.  
> Those are planned for **v2 and v3** respectively.

---

## ğŸ§  Key Features (v1)

- ğŸ“Œ Expected Knowledge analysis  
- ğŸ§  Demonstrated Understanding breakdown  
- âš ï¸ Identified knowledge gaps  
- ğŸ“Š Interview Risk Assessment  
- ğŸ¯ Confidence Score (0â€“100)  
- ğŸš€ Improvement Suggestions  
- âœ… Final Evaluation Summary  
- âœ¨ Smooth UI animations & loading state  

---

## ğŸ—ï¸ Tech Stack

### Backend
- **FastAPI** â€“ API framework
- **Python** â€“ Core logic
- **Groq LLM API** â€“ Model inference
- **Prompt Engineering** â€“ Controlled outputs

### Frontend
- **HTML / CSS / JavaScript**
- Custom animations & transitions
- Sticky confidence score
- Loading spinner for evaluations

## ğŸ“ Project Structure
```text
IntraVue-v1/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ config.py              # Model & prompt configuration
â”‚   â”œâ”€â”€ prompts/               # Prompt templates
â”‚   â”‚   â”œâ”€â”€ system_prompt.txt
â”‚   â”‚   â”œâ”€â”€ user_prompt.txt
â”‚   â”‚   â””â”€â”€ output_format.txt
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ evaluator.py       # Orchestrates evaluation
â”‚       â”œâ”€â”€ prompt_builder.py  # Builds structured prompts
â”‚       â””â”€â”€ llm_client.py      # Groq LLM client
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html             # UI structure
â”‚   â”œâ”€â”€ styles.css             # Styling & animations
â”‚   â””â”€â”€ script.js              # Client-side logic
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ§© Project Architecture
## âš™ï¸ How IntraVue Works (v1 Flow)

1. The user enters:
   - Job role
   - Experience level
   - Job description
   - Interview question
   - Candidate answer

2. The frontend sends the data to the backend endpoint:
   - Post / Evaluate

3. The backend:
   - Builds a structured prompt using job and candidate context
   - Enforces strict output formatting rules
   - Sends the request to the LLM

4. The LLM responds with:
   - Sectioned interview feedback
   - A numeric confidence score

5. The frontend:
   - Parses each section from the response
   - Animates and displays results
   - Visualizes the confidence score

---

## ğŸ“Š Evaluation Output Format

The LLM is forced to respond in the following structure:
Expected Knowledge:

...

Demonstrated Understanding:

...

Identified Gaps:

...

Interview Risk Assessment:

...

Confidence Score:
<number between 0 and 100>

Improvement Suggestions:

...

Evaluation Summary:
...

This guarantees consistent parsing and UX stability.

---

## â–¶ï¸ Running the Project Locally

### Backend Setup

  - cd backend
  - pip install -r requirements.txt
  - export GROQ_API_KEY=your_api_key
  - uvicorn app:app --reload

Backend runs at:

 - http://127.0.0.1:8000

### Frontend

Open the following file directly in your browser:

frontend/index.html

---

## ğŸ§ª Example Use Case

Interview Question:
What is HTML?

Evaluation Result:
- Knowledge depth assessed
- Missing concepts highlighted
- Interview risk inferred
- Confidence score generated

This simulates a real interview evaluation scenario.

---

## ğŸ›£ï¸ Roadmap

### v2 (Planned)
- Retrieval-Augmented Generation (RAG)
- Domain-specific evaluation
- Resume and JD grounding

### v3 (Planned)
- Agentic AI interviewers
- Multi-question interviews
- Skill-wise scoring
- Decision explanations

---


## â­ Final Note

IntraVue v1 is designed to be:
- Interview-ready
- Extendable
- Cleanly versioned
- Product-oriented

If you like this project, feel free to star the repository.
